@PREAMBLE{
 "\providecommand{\noopsort}[1]{}" 
 # "\providecommand{\singleletter}[1]{#1}%" 
}

@article{ref1,
author = {Tayebi, Zahra and Ali, Sarwan and Patterson, Murray},
title = {Robust Representation and Efficient Feature Selection Allows for Effective Clustering of SARS-CoV-2 Variants},
journal = {Algorithms},
volume = {14},
year = {2021},
number = {12},
url = {https://www.mdpi.com/1999-4893/14/12/348},
issn = {1999-4893},
abstract = {The widespread availability of large amounts of genomic data on the SARS-CoV-2 virus, as a result of the COVID-19 pandemic, has created an opportunity for researchers to analyze the disease at a level of detail, unlike any virus before it. On the one hand, this will help biologists, policymakers, and other authorities to make timely and appropriate decisions to control the spread of the coronavirus. On the other hand, such studies will help to more effectively deal with any possible future pandemic. Since the SARS-CoV-2 virus contains different variants, each of them having different mutations, performing any analysis on such data becomes a difficult task, given the size of the data. It is well known that much of the variation in the SARS-CoV-2 genome happens disproportionately in the spike region of the genome sequence&mdash;the relatively short region which codes for the spike protein(s). In this paper, we propose a robust feature-vector representation of biological sequences that, when combined with the appropriate feature selection method, allows different downstream clustering approaches to perform well on a variety of different measures. We use such proposed approach with an array of clustering techniques to cluster spike protein sequences in order to study the behavior of different known variants that are increasing at a very high rate throughout the world. We use a k-mers based approach first to generate a fixed-length feature vector representation of the spike sequences. We then show that we can efficiently and effectively cluster the spike sequences based on the different variants with the appropriate feature selection. Using a publicly available set of SARS-CoV-2 spike sequences, we perform clustering of these sequences using both hard and soft clustering methods and show that, with our feature selection methods, we can achieve higher F1 scores for the clusters and also better clustering quality metrics compared to baselines.},
doi = {10.3390/a14120348}
}

@book{ref42,
  title="Pattern recognition: A statistical approach",
  author="Devijver, Pierre A and Kittler, Josef",
  year="1982",
  publisher="Prentice-Hall",
  address="London, GB"
}

@inproceedings{ref35,
author = {Ali, Sarwan and Mansoor, Haris and Arshad, Naveed and Khan, Imdadullah},
title = {Short Term Load Forecasting Using Smart Meter Data},
year = {2019},
isbn = {9781450366717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307772.3330173},
doi = {10.1145/3307772.3330173},
abstract = {Accurate short term electricity load forecasting is crucial for efficient operations of the power sector. Predicting loads at a fine granularity (e.g. households) is made challenging due to a large number of (known or unknown) factors affecting power consumption. At larger scales (e.g. clusters of consumers), since the inherent stochasticity and fluctuations are averaged out, the problem becomes substantially easier. In this work we propose a method for short term (e.g. hourly) load forecasting at fine scale (households). Our method use hourly consumption data for a certain period (e.g. previous year) and predict hourly loads for the next period (e.g. next 6 months). We do not use any non-calendar information, hence our technique is applicable to any locality and dataset. We evaluate effectiveness of our technique on three benchmark datasets from Sweden, Australia, and Ireland.},
booktitle = {Proceedings of the Tenth ACM International Conference on Future Energy Systems},
pages = {419–421},
numpages = {3},
keywords = {Data Transformation, Load Forecasting, Clustering, SVD},
location = {Phoenix, AZ, USA},
series = {e-Energy '19}
}

@misc{ref43,
      title={Short-Term Load Forecasting Using AMI Data}, 
      author={Haris Mansoor and Sarwan Ali and Imdadullah Khan and Naveed Arshad and Muhammad Asad Khan and Safiullah Faizullah},
      year={2022},
      eprint={1912.12479},
      archivePrefix={arXiv},
      primaryClass={eess.SP}
}

@inproceedings{ref44,
 author = {Rahimi, Ali and Recht, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Random Features for Large-Scale Kernel Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
 volume = {20},
 year = {2007}
}

@article{ref45,
author = {Arthur E. Hoerl, Robert W. Kannard and Kent F. Baldwin},
title = {Ridge regression: some simulations},
journal = {Communications in Statistics},
volume = {4},
number = {2},
pages = {105-123},
year = {1975},
publisher = {Taylor \& Francis},
doi = {10.1080/03610927508827232},
URL = {https://doi.org/10.1080/03610927508827232},
eprint = {https://doi.org/10.1080/03610927508827232}
}

@article{ref46,
author = {McDonald, Gary C.},
title = {Ridge regression},
journal = {WIREs Computational Statistics},
volume = {1},
number = {1},
pages = {93-100},
keywords = {ridge trace, least squares, mean square error, bayesian model, search estimator},
doi = {https://doi.org/10.1002/wics.14},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.14},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.14},
abstract = {Abstract Ridge regression is a popular parameter estimation method used to address the collinearity problem frequently arising in multiple linear regression. The formulation of the ridge methodology is reviewed and properties of the ridge estimates capsulated. In particular, four rationales leading to a regression estimator of the ridge form are summarized. Algebraic properties of the ridge regression coefficients are given, which elucidate the behavior of a ridge trace for small values of the ridge parameter (i.e., close to the least squares solution) and for large values of the ridge parameter. Further properties involving coefficient sign changes and rates-of-change, as functions of the ridge parameter, are given for specific correlation structures among the independent variables. These results help relate the visual behavior of a ridge trace to the underlying structure of the data. Copyright © 2009 John Wiley \& Sons, Inc. This article is categorized under: Statistical Models > Linear Models Algorithms and Computational Methods > Least Squares},
year = {2009}
}

@INPROCEEDINGS{ref47,
  author={Muthukrishnan, R and Rohini, R},
  booktitle={2016 IEEE International Conference on Advances in Computer Applications (ICACA)}, 
  title={LASSO: A feature selection technique in predictive modeling for machine learning}, 
  year={2016},
  volume={},
  number={},
  pages={18-20},
  doi={10.1109/ICACA.2016.7887916}
}

@article{ref48,
 title={Feature Selection with the Boruta Package},
 volume={36},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v036i11},
 doi={10.18637/jss.v036.i11},
 number={11},
 journal={Journal of Statistical Software},
 author={Kursa, Miron B. and Rudnicki, Witold R.},
 year={2010},
 pages={1–13}
}

@article{ref49,
author={Fahim, A. M.
and Salem, A. M.
and Torkey, F. A.
and Ramadan, M. A.},
title={An efficient enhanced k-means clustering algorithm},
journal={Journal of Zhejiang University-SCIENCE A},
year={2006},
month={Oct},
volume={7},
number={10},
pages={1626-1633},
issn={1862-1775},
doi={10.1631/jzus.2006.A1626},
url={https://doi.org/10.1631/jzus.2006.A1626}
}

@article{ref50,
title = {Cluster center initialization algorithm for K-modes clustering},
journal = {Expert Systems with Applications},
volume = {40},
number = {18},
pages = {7444-7456},
year = {2013},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2013.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0957417413004648},
author = {Shehroz S. Khan and Amir Ahmad},
keywords = {K-modes clustering, Cluster center initialization, Prominent attributes, Significant attributes},
}

@article{ref51,
title = {FCM: The fuzzy c-means clustering algorithm},
journal = {Computers \& Geosciences},
volume = {10},
number = {2},
pages = {191-203},
year = {1984},
issn = {0098-3004},
doi = {https://doi.org/10.1016/0098-3004(84)90020-7},
url = {https://www.sciencedirect.com/science/article/pii/0098300484900207},
author = {James C. Bezdek and Robert Ehrlich and William Full},
keywords = {Cluster analysis, Cluster validity, Fuzzy clustering, Fuzzy QMODEL, Least-squared errors},
}

@software{ref52,
  author       = {Madson Luiz Dantas Dias},
  title        = {fuzzy-c-means: An implementation of Fuzzy $C$-means clustering algorithm.},
  month        = may,
  year         = 2019,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.3066222},
  url          = {https://git.io/fuzzy-c-means}
}

@InProceedings{ref53,
author="Campello, Ricardo J. G. B.
and Moulavi, Davoud
and Sander, Joerg",
editor="Pei, Jian
and Tseng, Vincent S.
and Cao, Longbing
and Motoda, Hiroshi
and Xu, Guandong",
title="Density-Based Clustering Based on Hierarchical Density Estimates",
booktitle="Advances in Knowledge Discovery and Data Mining",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="160--172",
isbn="978-3-642-37456-2"
}

@article{ref54,
doi = {10.21105/joss.00205},
url = {https://doi.org/10.21105/joss.00205},
year = {2017},
publisher = {The Open Journal},
volume = {2},
number = {11},
pages = {205},
author = {Leland McInnes and John Healy and Steve Astels},
title = {hdbscan: Hierarchical density based clustering},
journal = {Journal of Open Source Software}
}

@article{ref55,
title = {Efficient agglomerative hierarchical clustering},
journal = {Expert Systems with Applications},
volume = {42},
number = {5},
pages = {2785-2797},
year = {2015},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2014.09.054},
url = {https://www.sciencedirect.com/science/article/pii/S0957417414006150},
author = {Athman Bouguettaya and Qi Yu and Xumin Liu and Xiangmin Zhou and Andy Song},
keywords = {Clustering analysis, Hybrid clustering, Data mining, Data distribution, Coefficient of correlation},
abstract = {Hierarchical clustering is of great importance in data analytics especially because of the exponential growth of real-world data. Often these data are unlabelled and there is little prior domain knowledge available. One challenge in handling these huge data collections is the computational cost. In this paper, we aim to improve the efficiency by introducing a set of methods of agglomerative hierarchical clustering. Instead of building cluster hierarchies based on raw data points, our approach builds a hierarchy based on a group of centroids. These centroids represent a group of adjacent points in the data space. By this approach, feature extraction or dimensionality reduction is not required. To evaluate our approach, we have conducted a comprehensive experimental study. We tested the approach with different clustering methods (i.e., UPGMA and SLINK), data distributions, (i.e., normal and uniform), and distance measures (i.e., Euclidean and Canberra). The experimental results indicate that, using the centroid based approach, computational cost can be significantly reduced without compromising the clustering performance. The performance of this approach is relatively consistent regardless the variation of the settings, i.e., clustering methods, data distributions, and distance measures.}
}
