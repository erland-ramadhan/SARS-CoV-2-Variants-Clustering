% ****** Start of file aipsamp.tex ******
%
%   This file is part of the AIP files in the AIP distribution for REVTeX 4.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 American Institute of Physics.
%
%   See the AIP README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex  aipsamp
%  2)  bibtex aipsamp
%  3)  latex  aipsamp
%  4)  latex  aipsamp
%
% Use this file as a source of example code for your aip document.
% Use the file aiptemplate.tex as a template for your document.
\documentclass[%
 aip,
 jmp,%
 amsmath,amssymb,
%preprint,%
 reprint,%
%author-year,%
%author-numerical,%
]{revtex4-2}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{booktabs}
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

\begin{document}

\preprint{AIP/123-QED}

\title[Midterm Project]
{Robust Representation and Efficient Feature
Selection Allows for Effective Clustering of SARS-CoV-2
Variants:\\
Paper Reproduction dan Improvements\footnote{Error!}}% Force line breaks with \\
% \thanks{Footnote to title of article.}

\author{Erland Rachmad Ramadhan}
 % \altaffiliation[Also at ]{Physics Department, XYZ University.}%Lines break automatically or can be forced with \\
% \author{B. Author}%
 \email{mwkerr1916@icloud.com}
\affiliation{ 
Master Program of Mathematics, Department of Mathematics,\\
Faculty of Mathematics and Natural Science,\\
Universitas Indonesia%\\This line break forced with \textbackslash\textbackslash
}%

% \author{C. Author}
 % \homepage{http://www.Second.institution.edu/~Charlie.Author.}
% \affiliation{%
% Second institution and/or address%\\This line break forced% with \\
% }%

\date{4 November 2023}% It is always \today, today,
             %  but any date may be explicitly specified

\begin{abstract}
    The COVID-19 pandemic has led to an abundance of genomic data on the SARS-CoV-2 virus,
    presenting a unique opportunity for detailed analysis. This research benefits biologists,
    policymakers, and authorities in making informed decisions to control virus spread and
    prepares for future pandemics. Despite the challenge posed by the virus's diverse variants
    and mutations, this paper focuses on clustering spike protein sequences, crucial for
    understanding variant behavior. Utilizing a k-mers based approach, the original author of the paper
    create a fixed-length feature vector for spike sequences. The proposed feature selection
    method enables efficient clustering of spike sequences, demonstrating higher $F_1$ scores
    for clusters in both hard and soft clustering methods.
\end{abstract}

\keywords{COVID-19; SARS-CoV-2; Spike Protein Sequences;
Clustering Analysis; Feature Selection; k-mers}%Use showkeys class option if keyword
                              %display desired
\maketitle

%\begin{quotation}
%The ``lead paragraph'' is encapsulated with the \LaTeX\ 
%\verb+quotation+ environment and is formatted as a single paragraph before the first section heading. 
%(The \verb+quotation+ environment reverts to its usual meaning after the first sectioning command.) 
%Note that numbered references are allowed in the lead paragraph.
%%
%The lead paragraph will only be found in an article being prepared for the journal \textit{Chaos}.
%\end{quotation}

% \section{\label{sec:level1}First-level heading:\protect\\ The line
% break was forced \lowercase{via} \textbackslash\textbackslash}

\section{Introduction}
The SARS-CoV-2 virus, responsible for COVID-19, has a rapidly spreading genomic sequence worldwide.
This genetic information is crucial for understanding outbreak dynamics, designing analyses, drugs,
and vaccines, and monitoring changes in viral effectiveness over time. The virus's spike protein,
particularly its S region, plays a key role in infection and exhibits significant genomic variation.
To efficiently analyze this variation, the original author of the paper propose a focus on amino acid sequences
encoded by the spike region using machine learning and clustering methods \cite{ref1}. By converting these sequences
into numeric vectors through k-mers, the original author of the paper aim to reduce data dimensionality and enhance
analysis efficiency. The proposed approach integrates feature selection and clustering to gain insights
into the virus's evolutionary dynamics, overcoming challenges posed by the vast number of available genomic
sequences. The significance of the S protein makes it a potential target for therapeutic interventions and
vaccine development. The methodology proposed ensures meaningful analytics, laying the foundation for effective
strategies to combat the COVID-19 pandemic.

\section{Algorithms}
In this section, the proposed algorithm is discussed in detail. The discussion start with the
description of $k$-mers generation from the spike sequences. Then, the generation of
feature vector representation from the $k$-mers information will be described. After that,
discussion on different feature selection methods will be given in detail. Finally, the
detail of the application of clustering approaches on the final feature vector represetation
will be explained.

\subsection{k-mers Generation}
Given a spike sequence, the first step is to compute all possible $k$-mers. The total number of $k$-mers
that can be generated for a spike sequence are described as follows.
\begin{equation}
    N - k + 1
\end{equation}
where $N$ is the length of the spike sequence ($N=1274$ for this paper dataset). The variable $k$ is a
user-defined parameter ($k=3$ is chosen using standard validation set approach \cite{ref42}).

\subsection{Fixed-Length Feature Vector Generation}
Since most of the Machine Learning (ML) models work with a fixed-length feature vector representation,
the $k$-mers information is needed to be converted into the vectors. For this purpose, a feature vector
$\Phi_k$ is generated for a given spike sequence $a$ (i.e., $\Phi_k(a)$). Given an alphabet $\Sigma$
(characters representing amino acids in the spike sequences), the length of $\Phi_k(a)$ will be equal to
the number of possible $k$-mers of $a$. More formally,
\begin{equation}
    \Phi_k(a)=|\Sigma|^k
\end{equation}

Since there are 21 unique characters in $\Sigma$ (namely \emph{ACDEFGHIKLMNPQRSTVWXY}), the length
of each frequency vector is $21^3=9261$.

\subsection{Low Dimensional Representation}
Since the dimensionality of data is high after getting the fixed length feature vector representation,
different supervised and unsupervised methods is applied to obtain a low dimensional representation of
data to avoid the problem of the \emph{curse of dimensionality} \cite{ref35, ref43}. Each of the methods for
obtaining a low dimensional representation of data is discussed below:

\subsubsection{Random Fourier Features}
The first method that is used is an approximate kernel method called Random Fourier Features (RFF) \cite{ref44}.
It is an unsupervised approach, which maps the input data to a randomized low dimensional feature space
(euclidean inner product space) to get an approximate representation of data in lower dimensions $D$ from the
original dimensions $d$. More formally:
\begin{equation}
    z:\mathbb{R}^d\rightarrow\mathbb{R}^D
\end{equation}
In this way, the inner product between a pair of transformed points is approximated. More formally:
\begin{equation}
    f(x,y)=\langle\phi(x),\phi(y)\rangle\approx z(x)'z(y)
    \label{eq:eq4}
\end{equation}
In Equation \ref{eq:eq4}, $z$ is low dimensional (unlike the lifting $\phi$). Now, $z$ acts as the
approximate low dimensional embedding for the original data. Then, $z$ can be used as an input for
different ML tasks like clustering and classification.

\subsubsection{Least Absolute Shrinkage and Selection Operator (Lasso) Regression}
Lasso regression is a supervised method that can be used for efficient feature selection. It is a type of
regularized linear regression variants. It is a specific case of the penalized least squares regression with
an $L_1$ penalty function. By combining the good qualities of ridge regression \cite{ref45, ref46} and subset
selection, Lasso can improve both model interpretability and prediction accuracy \cite{ref47}. Lasso regression
tries to minimize the following objective function:
\begin{equation}
    \min(\text{Sum of square residuals} + \alpha\times|\text{slope}|)
\end{equation}
where $\alpha\times|\text{slope}|$ is the penalty term. In Lasso regression, the absolute value of the slope is
chosen in the penalty term rather than the square (as in ridge regression \cite{ref46}). This helps to reduce the
slope of useless variables exactly equal to zero.

\subsubsection{Boruta}
The last feature selection method that is used is Boruta. It is a supervised method that is made all around the
random forest (RF) classification algorithm. It works by creating shadow features so that the features do not compete
among themselves but rather they compete with a randomized version of them \cite{ref48}. It captures the non-linear
relationships and interactions using the RF algorithm. It then extract the importance of each feature (corresponding
to the class label) and only keep the features that are above a specific threshold of importance. The threshold is
defined as the highest feature importance recorded among the shadow features.

\subsection{Clustering Methods}
In the original work, five different clustering methods (both hard and soft clustering approaches) namely k-means\cite{ref49}, k-modes\cite{ref50}, Fuzzy
c-means\cite{ref51,ref52}, agglomerative hierarchical clustering, and Hierarchical density-based spatial clustering of applications with noise (HDBSCAN)\cite{ref53,ref54}
(note that is is a soft clustering approach). For the k-means and k-modes, default parameters are used. For the fuzzy c-means, the clustering criterion used to aggregate
subsets is a generalized least-squares objective function. For agglomerative hierarchical clustering, a bottom-up approach is applied, which is acknowledged as the
agglomerative method. Since the bottom-up procedure starts from anywhere in the central point of the hierarchy and the lower part of the hierarchy is developed by a
less expensive method such as partitional clustering, it can reduce the computational cost \cite{ref55}.

HDBSCAN is not just density-based spatial clustering of applications with noise (DBSCAN) but switching it into a hierarchical clustering algorithm and then obtaining
a flat clustering based in the solidity of clusters. HDBSCAN is robust to parameter choice and can discover clusters of differing densities (unlike DBSCAN)\cite{ref54}.

\section{Experimental Setup and Data Preparation}
In this paper reproduction, the author utilizes only three clustering methods—k-means, k-modes, and Fuzzy c-means.
HDBSCAN and agglomerative clustering are omitted due to their lack of parallelization, resulting in prolonged runtime
execution, particularly for high-dimensional data such as genome sequences or fixed-length feature vector representations
derived from raw data. For all clustering methods except k-means, the input data consists of low-dimensional
representations obtained from processing the raw data. This approach is adopted to address the extended runtime
execution associated with high-dimensional raw data input. The quality of the clustering algorithms is assessed using
the $F_1$ score. The experiments are conducted on a Core i7 system with a MacOS operating system, 16GB of memory, and a
1.7GHz processor. The algorithm is implemented in Python, and the code is accessible at
\url{https://github.com/erland-ramadhan/sars-cov2-variants-results-reproduction.git}.

\section{Results and Discussion}
\subsection{$F_1$ score}
The clustering quality was assessed using the weighted $F_1$ score. As cluster labels were unavailable,
each cluster was assigned a label based on the variant that had the majority of its sequences (e.g.,
assigning the label 'Alpha' to a cluster if most sequences belonged to the Alpha variant). Subsequently,
the weighted $F_1$ score was calculated individually for each cluster using these assigned labels. 
The reproduction work presents the computed weighted $F_1$ scores for different methods in Table \ref{tab:tab1},
while the original work provides them in Table \ref{tab:tab2}. The results indicate that Lasso regression outperforms
other feature selection methods in efficiently clustering all variants. In contrast to the original work,
where pure clusters of certain variants were observed with RFF, the reproduction work only observed
this phenomenon when using fuzzy c-means clustering. Therefore, k-means, k-modes, and fuzzy c-means exhibit better
generalization across variants when employing Lasso regression as the feature selection method.
\begin{table}
    \centering
    \caption{Variant-wise $F_1$ (weighted) score for different clustering methods. (Reproduction work)}
    \begin{tabular}{lccccc}
        % \hrule
        \toprule
        & \multicolumn{5}{c}{$F_1$ Score (Weighted) for Different Variants}\\
        \cmidrule{2-6}
        Methods & Alpha & Beta & Delta & Gamma & Epsilon \\
        \midrule
        \midrule
        K-Means            & 0.182 & 0.314 & 0.426 & 0.897 & 0.355\\
        K-Means + Boruta   & 0.182 & 0.314 & 0.423 & 0.897 & 0.355\\
        K-Means + Lasso    & 0.987 & 0.998 & 0.997 & 0.999 & 0.997\\ 
        K-Means + RFF      & 0.917 & 0.0 & 0.0 & 0.659 & 0.320\\ 
        \midrule
        K-Modes + Boruta   & 0.899 & 0.761 & 0.689 & 0.977 & 0.933\\
        K-Modes + Lasso    & 0.994 & 0.970 & 0.997 & 0.999 & 0.995\\ 
        K-Modes + RFF      & 0.178 & 0.0 & 0.0 & 0.981 & 0.320\\ 
        \midrule
        Fuzzy + Boruta   & 0.186 & 0.316 & 0.414 & 0.897 & 0.356\\
        Fuzzy + Lasso    & 0.976 & 0.998 & 0.985 & 0.997 & 0.972\\ 
        Fuzzy + RFF      & 1.0 & 0.0 & 0.0 & 0.659 & 0.0\\ 
        \bottomrule
    \end{tabular}
    \label{tab:tab1}
\end{table}

\begin{table}
    \centering
    \caption{Variant-wise $F_1$ (weighted) score for different clustering methods. (Original work)}
    \begin{tabular}{lccccc}
        % \hrule
        \toprule
        & \multicolumn{5}{c}{$F_1$ Score (Weighted) for Different Variants}\\
        \cmidrule{2-6}
        Methods & Alpha & Beta & Delta & Gamma & Epsilon \\
        \midrule
        \midrule
        K-Means            & 0.359 & 0.157 & 0.611 & 0.690 & 0.443\\
        K-Means + Boruta   & 0.418 & 0.105 & 0.610 & 0.690 & 0.652\\
        K-Means + Lasso    & 0.999 & 0.007 & 0.840 & 0.999 & 0.774\\ 
        K-Means + RFF      & 1.0 & 0.0 & 0.288 & 1.0 & 1.0\\ 
        \midrule
        K-Modes + Boruta   & 0.999 & 0.316 & 0.860 & 0.999 & 0.857\\
        K-Modes + Lasso    & 0.999 & 0.173 & 0.917 & 0.998 & 0.076\\ 
        K-Modes + RFF      & 1.0 & 0.0 & 0.0 & 0.613 & 1.0\\ 
        \midrule
        Fuzzy + Boruta   & 0.357 & 0.154 & 0.613 & 0.690 & 0.443\\
        Fuzzy + Lasso    & 0.999 & 0.314 & 0.647 & 0.999 & 0.816\\ 
        Fuzzy + RFF      & 0.439 & 0.0 & 0.0 & 1.0 & 0.0\\ 
        \bottomrule
    \end{tabular}
    \label{tab:tab2}
\end{table}

\subsection{Runtime Comparison}
After experimenting with various clustering methods and feature selection algorithms on spike sequences,
it was noted that k-means, k-modes, and fuzzy c-means exhibit superior performance when Lasso regression
is employed as the feature selection method, as evident from the weighted $F_1$ score. Nevertheless, it is
crucial to examine the runtime impact of these clustering approaches to assess the trade-off between $F_1$
score and runtime. To address this, the runtime for different clustering algorithms, both with and without
feature selection methods, was calculated and is presented in Table \ref{tab:tab3}. The analysis reveals
that k-modes is notably time-intensive, while k-means boasts the shortest execution time. This behavior
underscores the efficacy of the k-means algorithm, not only in terms of $F_1$ score but also in terms of
runtime efficiency.
\begin{table}
    \centering
    \caption{Running time (in seconds) of different clustering methods and feature selection methods.
    The number of clusters is 5 (Alpha, Beta, Delta, Gamma, and Epsilon).}
    \begin{tabular}{lcccc}
        \toprule
        & \multicolumn{4}{c}{Runtime for Different Feature Selection Methods}\\
        \cmidrule{2-5}
        Clustering Methods & No Feature Selection Method & Lasso & Boruta & RFF\\
        \midrule
        K-means & 153.211 & 2.029 & 2.353 & 2.802\\
        \midrule
        K-modes & N/A & 1485.150 & 1691.053 & 1501.018\\
        \midrule
        Fuzzy c-means & N/A & 66.509 & 89.783 & 198.234\\
        \bottomrule
    \end{tabular}
    \label{tab:tab3}
\end{table}

\section{Conclusion}
A proposed feature vector representation and a range of feature selection methods were introduced to
eliminate less significant features, enabling diverse clustering methods to effectively group SARS-CoV-2
spike protein sequences with high $F_1$ scores. The significance of runtime in clustering coronavirus spike
sequences was emphasized, with the k-means algorithm emerging as capable of both achieving pure clustering
across all variants and requiring the least runtime. Future work may involve expanding the dataset for
analysis, exploring additional clustering methods, and applying deep learning to larger datasets for more
nuanced insights. Another avenue is the exploration of alternative feature vector representations, such as
those based on minimizers, which can be implemented without sequence alignment. This extension could facilitate
the application of clustering techniques to study unaligned (even unassembled) sequencing reads of intra-host
viral populations, providing insights into dynamics at this scale.

\nocite{*}
% \bibliographystyle{plain}
\bibliography{aipsamp}% Produces the bibliography via BibTeX.

\end{document}
%
% ****** End of file aipsamp.tex ******
